{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Open the file and read lines\n",
    "with open('output.json', 'r') as file:\n",
    "    drugs_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drugs_data\n",
    "# Assuming 'data' is a list of dictionaries loaded from a JSON file as shown earlier.\n",
    "\n",
    "# We can calculate the average size of the 'uses' list, excluding drugs with an empty 'uses' list.\n",
    "# First, filter out the drugs with empty 'uses' list.\n",
    "filtered_data = [drug for drug in data if drug['uses']]\n",
    "\n",
    "# Now calculate the average size of the 'uses' list for the filtered drugs.\n",
    "total_uses = sum(len(drug['uses']) for drug in filtered_data)\n",
    "average_uses_non_empty = total_uses / len(filtered_data) if filtered_data else 0\n",
    "\n",
    "average_uses_non_empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def save_data(filename, data):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write('[\\n')  # Write the opening bracket of the JSON list\n",
    "        for i, entry in enumerate(data):\n",
    "            # Write each dictionary as a JSON string followed by a comma and newline, except for the last entry\n",
    "            if i < len(data) - 1:\n",
    "                file.write(json.dumps(entry, cls=NumpyEncoder) + ',\\n')\n",
    "            else:\n",
    "                # The last entry should not have a comma at the end\n",
    "                file.write(json.dumps(entry, cls=NumpyEncoder) + '\\n')\n",
    "        file.write(']')  # Write the closing bracket of the JSON list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Assuming the JSON data is a list of dictionaries\n",
    "# We will use a dictionary to remove duplicates based on a unique key in the dictionaries\n",
    "unique_data = {each_dict['name']: each_dict for each_dict in drugs_data}.values()\n",
    "for idx, drug in enumerate(unique_data):\n",
    "    drug['id'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('output2.json', unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a list of custom stop words (add more if needed)\n",
    "custom_stopwords = [\"and\", \"the\", \"in\", \"with\"]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove text in parentheses\n",
    "    \n",
    "    text_no_parentheses = re.sub(r'\\s*\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove commas\n",
    "    text_no_commas = text_no_parentheses.replace(',', '')\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = text_no_commas.split()\n",
    "    \n",
    "    # Remove stop words and custom stop words\n",
    "    cleaned_words = [word for word in words if word.lower() not in set(custom_stopwords)]\n",
    "    \n",
    "    # Join the cleaned words back into a string\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    if cleaned_text == '':\n",
    "        print(f'{text} was converted to {cleaned_text}')\n",
    "    # Return the cleaned text\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output2.json', 'r') as file:\n",
    "    drugs_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to drug names\n",
    "combined_features = [str((preprocess_text(drug['name']), drug['drug_classes'])) for drug in drugs_data]\n",
    "# Display the processed names\n",
    "#combined_features.sort()\n",
    "len(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = [re.sub(r\"[()',\\[\\]]\", \"\", entry) for entry in combined_features]\n",
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_drug_id_by_name(drug_name, drugs_data):\n",
    "    for drug in drugs_data:\n",
    "        if drug['name'] == drug_name:\n",
    "            return drug['id']\n",
    "    print('not found')\n",
    "    return None  # or raise an exception, or any other way you prefer to handle not found cases\n",
    "\n",
    "find_drug_id_by_name('', drugs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Vectorize the Names\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(combined_features)\n",
    "\n",
    "# Step 2: Find an Appropriate 'eps' Value\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "\n",
    "# Sort the distances\n",
    "sorted_distances = np.sort(distances, axis=0)\n",
    "sorted_distances = sorted_distances[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.title(\"Nearest Neighbors Distance\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to visually identify a suitable 'eps' value from this plot\n",
    "\n",
    "# Step 3: Apply DBSCAN Clustering\n",
    "# Assuming you chose an 'eps' value, for example, 0.3\n",
    "eps_value = .55 # adjust based on your observations\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=2)  # adjust min_samples as needed\n",
    "clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# Output the cluster assignments\n",
    "for name, cluster in zip(combined_features, clusters):\n",
    "    print(f\"Name: {name}, Cluster: {cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {}\n",
    "for name, cluster in zip(combined_features, clusters):\n",
    "    if cluster not in clusters_dict:\n",
    "        clusters_dict[cluster] = []\n",
    "    clusters_dict[cluster].append(name)\n",
    "\n",
    "# Now you can print the drugs for each cluster or work with them as needed\n",
    "for cluster, names in clusters_dict.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {entry['name']: name for entry, name in zip(drugs_data, combined_features)}\n",
    "# Add normalized names and cluster assignments to your data\n",
    "for entry, cluster in zip(drugs_data, clusters):\n",
    "    entry['Cluster'] = cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('o3.json', drugs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def consolidate_clusters(drugs_data):\n",
    "    # Group Data by Cluster, excluding -1\n",
    "    cluster_groups = defaultdict(list)\n",
    "    for drug in drugs_data:\n",
    "        if drug['Cluster'] != -1:\n",
    "            cluster_groups[drug['Cluster']].append(drug)\n",
    "    \n",
    "    # Consolidate Grouped Data\n",
    "    consolidated_data = []\n",
    "    for cluster, drugs in cluster_groups.items():\n",
    "        if drugs:\n",
    "            # Use the first drug's data as the template\n",
    "            consolidated_entry = drugs[0].copy()\n",
    "            # Replace name with the most common name in the cluster\n",
    "            names = [drug['name'] for drug in drugs]\n",
    "            consolidated_entry['name'] = min(set(names), key=names.count)\n",
    "            # Optionally, combine other attributes here\n",
    "            consolidated_entry['uses'] = list(set(use for drug in drugs for use in drug.get('uses', [])))\n",
    "            consolidated_entry['drug_classes'] = list(set(use for drug in drugs for use in drug.get('drug_classes', [])))\n",
    "            \n",
    "            consolidated_data.append(consolidated_entry)\n",
    "    \n",
    "    # Include the entries with Cluster -1 as they are\n",
    "    consolidated_data.extend(drug for drug in drugs_data if drug['Cluster'] == -1)\n",
    "    \n",
    "    return consolidated_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `drugs_data` is your list of dictionaries with 'Cluster' and 'Drug Name' keys\n",
    "consolidated_drugs_data = consolidate_clusters(drugs_data)\n",
    "cleaned_drug_data = [drug for drug in consolidated_drugs_data if len(drug['uses']) > 0]\n",
    "# Define a JSON encoder subclass to convert numpy integers to Python integers\n",
    "save_data('consolidated_data.json', consolidated_drugs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_drug_data = [drug for drug in consolidated_drugs_data if len(drug['uses']) > 0]\n",
    "for drug in cleaned_drug_data:\n",
    "    drug['name'] = re.sub(r\"\\s*\\(.*?\\)\", \"\", drug['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('consolidated_data.json', cleaned_drug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy \n",
    "import nmslib\n",
    "# Load the scispaCy model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Process a sample text\n",
    "text = \"Androgel Testosterone testosterone testosterone alcohol isopropyl myristate water sodium hydroxide CARBOMER HOMOPOLYMER TYPE C (ALLYL PENTAERYTHRITOL CROSSLINKED)\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities recognized as DRUG\n",
    "drugs = [ent for ent in doc.ents]\n",
    "for drug in drugs:\n",
    "    print(drug.text)\n",
    "    print(drug.label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "doc = nlp(\"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "           inherited motor neuron disease caused by the expansion \\\n",
    "           of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "           SBMA can be caused by this easily. Attention Deficit Hyperactivity Disorder (ADHD) is also a problem\")\n",
    "\n",
    "print(\"Abbreviation\", \"\\t\", \"Definition\")\n",
    "for abrv in doc._.abbreviations:\n",
    "\tprint(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# This line takes a while, because we have to download ~1GB of data\n",
    "# and load a large JSON file (the knowledge base). Be patient!\n",
    "# Thankfully it should be faster after the first time you use it, because\n",
    "# the downloads are cached.\n",
    "# NOTE: The resolve_abbreviations parameter is optional, and requires that\n",
    "# the AbbreviationDetector pipe has already been added to the pipeline. Adding\n",
    "# the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n",
    "# that linking will only be performed on the long form of abbreviations.\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "doc = nlp(\"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "           inherited motor neuron disease caused by the expansion \\\n",
    "           of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "           SBMA can be caused by this easily. Attention deficit Hyperactivity Disorder (ADHD) is also a problem\")\n",
    "\n",
    "# Let's look at a random entity!\n",
    "entity = doc.ents[1]\n",
    "\n",
    "print(\"Name: \", entity)\n",
    "# >>> Name: bulbar muscular atrophy\n",
    "\n",
    "# Each entity is linked to UMLS with a score\n",
    "# (currently just char-3gram matching).\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "for umls_ent in entity._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2 = doc.ents[12]\n",
    "print(\"Name: \", entity2)\n",
    "# >>> Name: bulbar muscular atrophy\n",
    "\n",
    "# Each entity is linked to UMLS with a score\n",
    "# (currently just char-3gram matching).\n",
    "for umls_ent in entity2._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2 = doc.ents[11]\n",
    "print(\"Name: \", entity2)\n",
    "# >>> Name: bulbar muscular atrophy\n",
    "\n",
    "# Each entity is linked to UMLS with a score\n",
    "# (currently just char-3gram matching).\n",
    "for umls_ent in entity2._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have ADHD. It's a huge problem. I also take adderall.\")\n",
    "\n",
    "\n",
    "for i in range(len(doc.ents)):\n",
    "    # Let's look at a random entity!\n",
    "    entity = doc.ents[i]\n",
    "\n",
    "    print(\"Name: \", entity)\n",
    "    # >>> Name: bulbar muscular atrophy\n",
    "\n",
    "    # Each entity is linked to UMLS with a score\n",
    "    # (currently just char-3gram matching).\n",
    "    for umls_ent in entity._.kb_ents:\n",
    "        print(linker.kb.cui_to_entity[umls_ent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_effects = [\"sweating\", \"nausea/vomiting, chemotherapy induced\", \"agitation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sjchu\\.pyenv\\pyenv-win\\versions\\3.9.6\\lib\\site-packages\\spacy\\language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "# Obtain the linker from the pipeline\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T033']\n",
      "['T184']\n",
      "['T184']\n",
      "['T061']\n",
      "['T169']\n",
      "['T184']\n",
      "['Sweating', 'Nausea', 'Vomiting', 'Agitation']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "def normalize_side_effects(side_effects_list, nlp, linker):\n",
    "    side_effects_text = \", \".join(side_effects_list) + \".\"\n",
    "    doc = nlp(side_effects_text)\n",
    "\n",
    "    normalized_effects = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        kb_ents = ent._.kb_ents\n",
    "        if kb_ents:\n",
    "            cui = kb_ents[0][0]\n",
    "            umls_entity = linker.kb.cui_to_entity[cui]\n",
    "            semantic_types = umls_entity.types  # Semantic types of the UMLS entity\n",
    "            print(semantic_types)\n",
    "            # Use semantic types to filter diseases or symptoms\n",
    "            if \"T047\" in semantic_types or \"T184\" in semantic_types or \"T033\" in semantic_types:  # T047: Disease, T184: Sign or Symptom\n",
    "                normalized_name = umls_entity.canonical_name\n",
    "                normalized_effects.append(normalized_name)\n",
    "\n",
    "    return normalized_effects\n",
    "\n",
    "\n",
    "# Example usage\n",
    "side_effects = [\"sweating\", \"nausea\", \"vomiting\", \"chemotherapy induced\", \"agitation\"]\n",
    "normalized_side_effects = normalize_side_effects(side_effects, nlp, linker)\n",
    "print(normalized_side_effects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lidocaine: ['T109', 'T121']\n",
      "Human body: ['T016']\n",
      "Hypesthesia: ['T033']\n",
      "Loss: ['T081']\n",
      "Patients: ['T101']\n",
      "Medical procedure: ['T058']\n",
      "Pain: ['T184']\n",
      "Pruritus: ['T184']\n",
      "Condition: ['T080']\n",
      "Sunburn: ['T037']\n",
      "Burn injury: ['T037']\n",
      "Insecta: ['T204']\n",
      "bite injury: ['T037']\n",
      "Sting Injury: ['T037']\n",
      "Poison Sumac: ['T002']\n",
      "Dermatitis verrucosa: ['T047']\n",
      "Xylocaine Jelly: ['T109', 'T121']\n",
      "Treatment intent: ['T169']\n",
      "Pain: ['T184']\n",
      "Inflammation: ['T046']\n",
      "Urethra: ['T023']\n",
      "PREVENT (product): ['T121']\n",
      "Pain management: ['T061']\n",
      "Methods aspects: ['T169']\n",
      "Males: ['T032']\n",
      "Female urethral structure: ['T023']\n",
      "Xylocaine Jelly: ['T109', 'T121']\n",
      "Nose: ['T023']\n",
      "Oral cavity: ['T030']\n",
      "Pharyngeal structure: ['T023']\n",
      "Intubation (procedure): ['T061']\n",
      "['Dermatitis verrucosa', 'Hypesthesia', 'Burn injury', 'Inflammation', 'Sunburn', 'Pain', 'bite injury', 'Sting Injury', 'Pruritus']\n"
     ]
    }
   ],
   "source": [
    "text = \"Lidocaine topical jelly or ointment is used on different parts of the body to cause numbness or loss of feeling for patients having certain medical procedures. It is also used to relieve pain and itching caused by conditions such as sunburn or other minor burns, insect bites or stings, poison ivy, poison oak, poison sumac, minor cuts, or scratches. Xylocaine® jelly is used to treat painful urethritis (inflammation of the urethra). It is also used to prevent and control pain in procedures involving the male and female urethra. Xylocaine® jelly is also used to lubricate the nose, mouth, and throat for intubation.\"\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "# Initialize list for conditions and diseases\n",
    "conditions_and_diseases = []\n",
    "relevant_types =  {\"T047\", \"T184\", \"T033\", \"T037\", \"T046\",  \"T195\"}\n",
    "\n",
    "\n",
    "# Iterate over the entities and link them to UMLS\n",
    "for ent in doc.ents:\n",
    "    # Access the linked entities in UMLS\n",
    "    kb_ents = ent._.kb_ents\n",
    "    if kb_ents:\n",
    "        cui = kb_ents[0][0]  # Get the top CUI\n",
    "        score = kb_ents[0][1]  # Similarity score\n",
    "        umls_entity = linker.kb.cui_to_entity[cui]\n",
    "        semantic_types = umls_entity.types\n",
    "        print(f\"{umls_entity.canonical_name}: {semantic_types}\")\n",
    "        # Check if the UMLS entity is a 'Finding' (T033)\n",
    "        if any(st in relevant_types for st in umls_entity.types):\n",
    "            conditions_and_diseases.append(umls_entity.canonical_name)\n",
    "\n",
    "# Remove duplicates and print the list of conditions and diseases\n",
    "unique_conditions_and_diseases = list(set(conditions_and_diseases))\n",
    "print(unique_conditions_and_diseases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
